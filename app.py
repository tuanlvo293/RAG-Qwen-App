import streamlit as st
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.llms import HuggingFacePipeline

st.set_page_config(page_title="RAG Qwen PDF", layout="wide")
st.title("üìö H·ªá th·ªëng RAG - PhD Assistant")

@st.cache_resource
def load_llm():
    # S·ª≠ d·ª•ng b·∫£n 0.5B ƒë·ªÉ nh·∫π m√°y
    model_id = "Qwen/Qwen2.5-0.5B-Instruct" 
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", device_map="cpu")
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=128)
    return HuggingFacePipeline(pipeline=pipe)

llm = load_llm()

uploaded_file = st.file_uploader("T·∫£i l√™n t√†i li·ªáu PDF c·ªßa b·∫°n", type="pdf")

if uploaded_file:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.read())
    
    loader = PyPDFLoader("temp.pdf")
    docs = loader.load()
    
    # Chia nh·ªè vƒÉn b·∫£n theo ƒë√∫ng th√¥ng s·ªë Colab c·ªßa b·∫°n
    splitter = RecursiveCharacterTextSplitter(chunk_size=588, chunk_overlap=108)
    chunks = splitter.split_documents(docs)
    
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    st.success("T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c n·∫°p th√†nh c√¥ng!")
    
    question = st.text_input("C√¢u h·ªèi c·ªßa b·∫°n:")
    if question:
        retrieved_docs = vector_store.as_retriever(search_kwargs={"k": 4}).invoke(question)
        context = "\n".join(doc.page_content for doc in retrieved_docs)
        
        # Prompt chu·∫©n t·ª´ m√£ ngu·ªìn b·∫°n cung c·∫•p
        prompt = f"""<|im_start|>system
B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch. Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y m·ªôt c√°ch ng·∫Øn g·ªçn.
N·∫øu kh√¥ng th·∫•y ƒë·ªß th√¥ng tin, h√£y n√≥i b·∫°n kh√¥ng bi·∫øt. <|im_end|>
<|im_start|>user
Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {question}<|im_end|>
<|im_start|>assistant:"""

        response = llm.invoke(prompt)
        # T√°ch ƒë√°p √°n d·ª±a tr√™n c·∫•u tr√∫c Qwen
        ans = response.split("<|im_start|>assistant:")[-1].split("<|im_end|>")[0].strip()
        st.markdown(f"**Tr·∫£ l·ªùi:** {ans}")